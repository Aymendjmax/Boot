import os
import telebot
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote, urlencode
from threading import Thread, Lock
from flask import Flask
import time
import logging
import atexit
import re
import json
import random

# ุฅุนุฏุงุฏุงุช ุงูุณุฌู
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ุชููุฆุฉ Flask
app = Flask(__name__)

# ููู ููุนูููุงุช ุงูุญุฑุฌุฉ
bot_lock = Lock()

# ุงูุญุตูู ุนูู ุงููุชุบูุฑุงุช ุงูุจูุฆูุฉ
TOKEN = os.environ.get('TOKEN')
API_KEY = os.environ.get('API_KEY')
bot = telebot.TeleBot(TOKEN, threaded=True)

# ุงูููุงูุฌ ุงูุฏุฑุงุณูุฉ ููุณูุฉ ุงูุฑุงุจุนุฉ ูุชูุณุท
SUBJECTS_4AM = {
    "ุฑูุงุถูุงุช": ["ูุนุงุฏูุงุช", "ููุฏุณุฉ", "ุฅุญุตุงุก", "ุงุญุชูุงูุงุช", "ุฏูุงู", "ูุชุชุงููุงุช", "ูุซูุซุงุช", "ุชูุงุณุจ", "ุญุณุงุจ ูุซูุซู", "ูุธุฑูุฉ ููุซุงุบูุฑุณ", "ูุจุฑููุฉ ุทุงููุณ"],
    "ุนููู ุทุจูุนูุฉ": ["ูุธุงุฆู ุงูุญูุงุฉ", "ุงูุชููุณ", "ุงูุชูุงุซุฑ", "ุงูุฌูุงุฒ ุงูุนุตุจู", "ุงูุฌูุงุฒ ุงููุถูู", "ุงูุฌูุงุฒ ุงูุชููุณู", "ุงูุจูุฆุฉ", "ุงูุชููุซ", "ุงูุณูุงุณู ุงูุบุฐุงุฆูุฉ"],
    "ููุฒูุงุก": ["ุงูุถูุก", "ุงูููุฑุจุงุก", "ุงููุบูุงุทูุณ", "ุงูุทุงูุฉ", "ุงูุญุฑูุฉ", "ุงูููู", "ุงูุถุบุท", "ุงูุญุฑุงุฑุฉ", "ุงููุชูุฉ", "ุงูุญุฌู", "ุงููุซุงูุฉ"],
    "ุนุฑุจูุฉ": ["ููุงุนุฏ", "ุจูุงุบุฉ", "ุฅุนุฑุงุจ", "ูุตูุต ุฃุฏุจูุฉ", "ุดุนุฑ", "ูุซุฑ", "ุชุนุจูุฑ ูุชุงุจู", "ููุนูู ุจู", "ููุนูู ูุทูู", "ูุงุนู", "ูุจุชุฏุฃ", "ุฎุจุฑ"],
    "ูุฑูุณูุฉ": ["grammaire", "conjugaison", "vocabulaire", "expression รฉcrite", "lecture", "comprรฉhension"],
    "ุฅูุฌููุฒูุฉ": ["grammar", "vocabulary", "reading", "writing", "speaking", "listening"],
    "ุชุงุฑูุฎ": ["ุงูุซูุฑุฉ ุงูุฌุฒุงุฆุฑูุฉ", "ุงูุงุณุชุนูุงุฑ", "ุงูุญุฑุจ ุงูุนุงูููุฉ", "ุงูุญุฑูุฉ ุงููุทููุฉ", "ุงูููุงููุฉ ุงูุดุนุจูุฉ"],
    "ุฌุบุฑุงููุง": ["ุงูุชุถุงุฑูุณ", "ุงูููุงุฎ", "ุงูุณูุงู", "ุงูููุงุฑุฏ ุงูุทุจูุนูุฉ", "ุงูุฒุฑุงุนุฉ", "ุงูุตูุงุนุฉ", "ุงูุชุฌุงุฑุฉ"],
    "ุชุฑุจูุฉ ุฅุณูุงููุฉ": ["ุงูุนููุฏุฉ", "ุงูุนุจุงุฏุงุช", "ุงูุณูุฑุฉ ุงููุจููุฉ", "ุงูุฃุฎูุงู", "ุงููุนุงููุงุช", "ุงูุญุฏูุซ", "ุงููุฑุขู"],
    "ุชุฑุจูุฉ ูุฏููุฉ": ["ุงูููุงุทูุฉ", "ุงูุฏูููุฑุงุทูุฉ", "ุญููู ุงูุฅูุณุงู", "ุงููุฌุชูุน ุงููุฏูู", "ุงููุคุณุณุงุช"]
}

# ุงูููุงูุน ุฐุงุช ุงูุฃููููุฉ ููุจุญุซ
PRIORITY_SOURCES = {
    "DzExams": "https://dzexams.com/4am?s=",
    "Eddirasa": "https://www.eddirasa.com/search?q=",
    "ุงูุฏููุงู ุงููุทูู": "http://www.onefd.edu.dz/?s=",
    "ูุฒุงุฑุฉ ุงูุชุฑุจูุฉ": "http://www.education.gov.dz/?s=",
}

# ุจุงูู ุงููุตุงุฏุฑ ุงูููุฏูุฉ ูุน ุฑูุงุจุท ุงูุจุญุซ ุงูุฎุงุตุฉ ุจูุง
SECONDARY_SOURCES = {
    "ุงูููุชุจุฉ ุงูุฑูููุฉ": "http://elearning.mesrs.dz/search?q=",
    "ููุตุฉ ุงูุฌุฒุงุฆุฑ": "http://edu-dz.com/search?query=",
    "ุงูุฃุณุชุงุฐ ุงูุฌุฒุงุฆุฑู": "https://www.prof-dz.com/search?q=",
    "ุชุนููู ูุช": "https://www.taalimnet.com/search?query=",
    "ุทุงุณููู": "https://www.tassilialgerie.com/recherche?q=",
    "ูุฏุฑุณุชู": "https://madrassati.com/search?q=",
    "Ta3lim": "https://www.ta3lim.com/search?q=",
    "ูุฑูุถ dz": "https://www.frodz.com/search?q=",
    "ุงูุชุญุงูุงุช dz": "https://www.examens-dz.com/search?q=",
    "4AM Exams": "https://www.4am-exams.com/search?q=",
    "Moyennes": "https://www.moyennes-dz.com/search?q=",
    "Madrassati Exams": "https://madrassati-exams.com/search?q=",
    "ูุฏููุฉ ุงูุชุนููู": "https://education-algerie.blogspot.com/search?q=",
    "ูุฏููุฉ ุงููุฑูุถ": "https://frodj-4am.blogspot.com/search?q=",
    "ูุฏููุฉ ุงูุฃุณุชุงุฐ": "https://prof-algerien.blogspot.com/search?q="
}

# ูุตุงุฏุฑ ูุง ุชุญุชูู ุนูู ุตูุญุฉ ุจุญุซ
SPECIAL_SOURCES = [
    "https://4am.frodz.com",
    "https://www.examens-education.dz",
    "https://www.model-exams.com",
    "https://www.moyenne-exams.com",
    "https://www.old-exams.dz"
]

# ุงูุฃูุงูุฑ ุงูุนุฑุจูุฉ
ARABIC_COMMANDS = {
    'start': 'ุจุฏุก ุงูุชุดุบูู',
    'who': 'ูู ุฃูุช',
    'creator': 'ุงููุทูุฑ',
    'job': 'ูุธููุชู',
    'reset': 'ุฅุนุงุฏุฉ ุชุนููู',
    'search': 'ุจุญุซ ูู ุงููููุงุฌ',
    'youtube': 'ุจุญุซ ูู ููุชููุจ'
}

@app.route('/')
def home():
    return "EdoBot is running!"

@app.route('/health')
def health():
    return "OK", 200

def set_bot_commands():
    with bot_lock:
        commands = [
            telebot.types.BotCommand(cmd, desc) 
            for cmd, desc in ARABIC_COMMANDS.items()
        ]
        bot.set_my_commands(commands)

@bot.message_handler(commands=['start'])
def send_welcome(message):
    welcome_msg = """ูุฑุญุจุงู! ุฃูุง EdoBot ูุณุงุนุฏู ุงูุฏุฑุงุณู ๐

ุงุณุชุฎุฏู ูุฐู ุงูุฃูุงูุฑ:
/who - ููุชุนุฑูู ุจู
/creator - ููุนุฑูุฉ ุงููุทูุฑ
/job - ููุนุฑูุฉ ูุธููุชู
/search - ููุจุญุซ ูู ุงููููุงุฌ
/youtube - ููุจุญุซ ุนู ููุฏูููุงุช ุชุนููููุฉ
/reset - ููุณุญ ุงููุญุงุฏุซุฉ

ููููู ุฃูุถูุง ุฅุฑุณุงู ุณุคุงูู ูุจุงุดุฑุฉ ูุณุฃุจุญุซ ูู ุงููููุงุฌ ุงูุฌุฒุงุฆุฑู ูุฑุงุจุนุฉ ูุชูุณุท"""
    bot.send_message(message.chat.id, welcome_msg)

@bot.message_handler(commands=['who'])
def who_are_you(message):
    with bot_lock:
        bot.reply_to(message, "ุฃูุง EdoBotุ ุฑูุจูุช ูุณุงุนุฏ ูุทูุงุจ ุงูุณูุฉ ุงูุฑุงุจุนุฉ ูุชูุณุท ูู ุงูุฌุฒุงุฆุฑ ๐")

@bot.message_handler(commands=['creator'])
def who_created_you(message):
    with bot_lock:
        bot.reply_to(message, "ุตูููู ุงููุทูุฑ Aymen dj max. ๐\nุฒูุฑูุง ูููุนู: adm-web.ct.ws")

@bot.message_handler(commands=['job'])
def your_job(message):
    with bot_lock:
        bot.reply_to(message, "ูุธููุชู ูุณุงุนุฏุชู ูู:\n- ุญู ุฃุณุฆูุฉ ุงููููุงุฌ\n- ุดุฑุญ ุงูุฏุฑูุณ\n- ุชูููุฑ ูุตุงุฏุฑ ููุซููุฉ\n- ุงูุจุญุซ ุนู ููุฏูููุงุช ุชุนููููุฉ")

@bot.message_handler(commands=['reset'])
def reset_chat(message):
    with bot_lock:
        bot.reply_to(message, "ุชู ุฅุนุงุฏุฉ ุชุนููู ุงูุฏุฑุฏุดุฉ ุจูุฌุงุญ โ")

@bot.message_handler(commands=['search'])
def handle_search(message):
    with bot_lock:
        msg = bot.reply_to(message, "ุฃุฏุฎู ุณุคุงูู ุงูุฏุฑุงุณู:")
        bot.register_next_step_handler(msg, process_search)

@bot.message_handler(commands=['youtube'])
def handle_youtube_search(message):
    with bot_lock:
        msg = bot.reply_to(message, "ุฃุฏุฎู ููุถูุน ุงูุจุญุซ ููุญุตูู ุนูู ููุฏูููุงุช ุชุนููููุฉ:")
        bot.register_next_step_handler(msg, process_youtube_search)

def process_search(message):
    try:
        handle_edu_question(message)
    except Exception as e:
        logger.error(f"Search error: {str(e)}")
        with bot_lock:
            bot.reply_to(message, "ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุงูุจุญุซ. ูุฑุฌู ุงููุญุงููุฉ ูุงุญููุง.")

def process_youtube_search(message):
    try:
        query = message.text + " ุณูุฉ ุฑุงุจุนุฉ ูุชูุณุท ุดุฑุญ ุฏุฑุณ"
        videos = search_youtube(query)
        
        if videos:
            response = "๐ฌ ููุฏูููุงุช ุชุนููููุฉ ูุชุนููุฉ ุจุงูููุถูุน:\n\n"
            for video in videos[:2]:
                response += f"๐บ {video['title']}\n"
                response += f"๐๏ธ {video['views']} ูุดุงูุฏุฉ\n"
                response += f"๐ {video['url']}\n\n"
            with bot_lock:
                bot.reply_to(message, response)
        else:
            with bot_lock:
                bot.reply_to(message, "ููุฃุณู ูู ุฃุฌุฏ ููุฏูููุงุช ููุงุณุจุฉ. ุญุงูู ุงุณุชุฎุฏุงู ูููุงุช ููุชุงุญูุฉ ุฃุฎุฑู.")
    except Exception as e:
        logger.error(f"YouTube search error: {str(e)}")
        with bot_lock:
            bot.reply_to(message, "ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุงูุจุญุซ ูู ููุชููุจ. ูุฑุฌู ุงููุญุงููุฉ ูุงุญููุง.")

def is_4am_curriculum_related(text):
    """ูุญุต ุฏููู ุฅุฐุง ูุงู ุงูุณุคุงู ูุชุนูู ุจูููุงุฌ ุงูุณูุฉ ุงูุฑุงุจุนุฉ ูุชูุณุท"""
    
    # ูุงุฆูุฉ ุจุงููููุงุช ุงูููุชุงุญูุฉ ูููููุงุฌ
    text_lower = text.lower()
    
    # ุชุญุฏูุฏ ุงููุบุฉ ุงูุฃุณุงุณูุฉ ููุณุคุงู
    is_arabic = bool(re.search(r'[\u0600-\u06FF]', text))
    is_french = bool(re.search(r'[รฉรจรชรซรรขรงรนรปรผรฟรดรฎรฏลรฆ]|^[a-z\s]+$', text_lower))
    is_english = bool(re.search(r'^[a-z\s\.,:;!\?]+$', text_lower))
    
    # ูููุงุช ุชุดูุฑ ุฅูู ุงูุณูุฉ ุงูุฑุงุจุนุฉ ูุชูุณุท
    grade_indicators = [
        "ุฑุงุจุนุฉ ูุชูุณุท", "4 ูุชูุณุท", "4am", "4eme", "4annรฉe", "quatriรจme", 
        "ุงูุณูุฉ ุงูุฑุงุจุนุฉ", "ุงูุณูุฉ ูค", "ุงูุณูุฉ 4", "ุตู ุฑุงุจุน", "ุตู ูค", "ุตู 4"
    ]
    
    # ุชุญูู ูู ุฅุดุงุฑุฉ ุฅูู ุงููุณุชูู ุงูุฏุฑุงุณู
    has_grade_indicator = any(indicator in text_lower for indicator in grade_indicators)
    
    # ุฅุฐุง ูู ูุฐูุฑ ุงููุณุชููุ ูุชุญูู ูู ูุญุชูู ุงูุณุคุงู
    if not has_grade_indicator:
        # ุงูุจุญุซ ูู ููุงุถูุน ูู ูุงุฏุฉ
        for subject, topics in SUBJECTS_4AM.items():
            # ูุญุต ุงุณู ุงููุงุฏุฉ
            if subject in text_lower:
                return True
                
            # ูุญุต ููุถูุนุงุช ุงููุงุฏุฉ
            for topic in topics:
                if topic in text_lower:
                    return True
    else:
        return True
    
    # ูุญุต ุฅุถุงูู ููุนุจุงุฑุงุช ุงูุชู ุชุดูุฑ ุฅูู ูุญุชูู ุฏุฑุงุณู
    educational_terms = [
        "ูุงุฌุจ", "ูุฑุถ", "ุงุฎุชุจุงุฑ", "ุงูุชุญุงู", "ุฏุฑุณ", "ุดุฑุญ", "ุชูุฑูู", "ุญู", 
        "ูุงููู", "ูุณุฃูุฉ", "ุณุคุงู", "ูููุงุฌ", "ุจุฑูุงูุฌ", "ุชุนููู", "ูุฑุงุฌุนุฉ",
        "devoir", "examen", "cours", "exercice", "solution", "problรจme",
        "homework", "test", "lesson", "exercise", "problem", "review"
    ]
    
    return any(term in text_lower for term in educational_terms)

def search_youtube(query):
    """ุงูุจุญุซ ูู ููุชููุจ ูุชุฑุชูุจ ุงููุชุงุฆุฌ ุญุณุจ ุนุฏุฏ ุงููุดุงูุฏุงุช"""
    try:
        # ุจูุงุก ุฑุงุจุท ุงูุจุญุซ
        search_query = quote(query)
        url = f"https://www.youtube.com/results?search_query={search_query}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            logger.error(f"YouTube search failed with status code: {response.status_code}")
            return []
            
        # ุงุณุชุฎุฑุงุฌ ุจูุงูุงุช ุงูููุฏูููุงุช ุจุงุณุชุฎุฏุงู BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ุงูุจุญุซ ุนู ุงูุจูุงูุงุช ุงููุถููุฉ ูู ุตูุญุฉ ุงูุจุญุซ (ุงูุทุฑููุฉ ุงูุฃูุซุฑ ููุซูููุฉ)
        videos = []
        
        # ุงุณุชุฎุฑุงุฌ ุงูุจูุงูุงุช ูู ุงููุต
        pattern = r'var ytInitialData = (.+?);</script>'
        matches = re.search(pattern, response.text)
        
        if not matches:
            # ุทุฑููุฉ ุจุฏููุฉ ููุจุญุซ
            video_elements = soup.select('div#contents ytd-video-renderer, div#contents ytd-compact-video-renderer')
            
            for element in video_elements[:5]:
                title_element = element.select_one('h3 a#video-title') or element.select_one('a#video-title')
                if not title_element:
                    continue
                    
                title = title_element.text.strip()
                video_id = title_element.get('href', '').split('?v=')[-1].split('&')[0]
                url = f"https://www.youtube.com/watch?v={video_id}"
                
                # ูุญุงููุฉ ุงุณุชุฎุฑุงุฌ ุนุฏุฏ ุงููุดุงูุฏุงุช
                views_element = element.select_one('span.style-scope.ytd-video-meta-block:contains("views")')
                views = views_element.text.strip() if views_element else "ุบูุฑ ูุนุฑูู"
                
                videos.append({
                    'title': title,
                    'url': url,
                    'views': views
                })
            
            # ุฅุฐุง ูู ูุฌุฏ ุฃู ููุฏูู ุจุงูุทุฑููุฉ ุงูุจุฏููุฉ
            if not videos:
                # ุงุณุชุฎุฏุงู ุทุฑููุฉ ุฃุฎุฑู ููุจุญุซ ุนู ุงูููุฏูููุงุช
                script_elements = soup.find_all('script')
                for script in script_elements:
                    if 'var ytInitialData' in script.text:
                        data_text = script.text.split('var ytInitialData = ')[1].split(';</script>')[0]
                        data = json.loads(data_text)
                        
                        # ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุงูููุฏูู ูู ุงูุจูุงูุงุช
                        try:
                            video_items = data['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents'][0]['itemSectionRenderer']['contents']
                            
                            for item in video_items:
                                if 'videoRenderer' in item:
                                    video_data = item['videoRenderer']
                                    video_id = video_data.get('videoId', '')
                                    
                                    if not video_id:
                                        continue
                                        
                                    title = video_data.get('title', {}).get('runs', [{}])[0].get('text', 'ุจุฏูู ุนููุงู')
                                    view_count_text = video_data.get('viewCountText', {}).get('simpleText', 'ุบูุฑ ูุนุฑูู')
                                    
                                    videos.append({
                                        'title': title,
                                        'url': f"https://www.youtube.com/watch?v={video_id}",
                                        'views': view_count_text
                                    })
                        except Exception as e:
                            logger.error(f"Error parsing YouTube data: {str(e)}")
        else:
            try:
                data = json.loads(matches.group(1))
                video_items = data['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents'][0]['itemSectionRenderer']['contents']
                
                for item in video_items:
                    if 'videoRenderer' in item:
                        video_data = item['videoRenderer']
                        video_id = video_data.get('videoId', '')
                        
                        if not video_id:
                            continue
                            
                        title = video_data.get('title', {}).get('runs', [{}])[0].get('text', 'ุจุฏูู ุนููุงู')
                        
                        # ุงุณุชุฎุฑุงุฌ ุนุฏุฏ ุงููุดุงูุฏุงุช
                        view_count_text = "ุบูุฑ ูุนุฑูู"
                        if 'viewCountText' in video_data:
                            view_count_text = video_data['viewCountText'].get('simpleText', 'ุบูุฑ ูุนุฑูู')
                            # ุชุญููู ุงููุต ุฅูู ุฑูู ููุชุฑุชูุจ
                            try:
                                view_count = ''.join(filter(str.isdigit, view_count_text))
                            except:
                                view_count = 0
                        
                        videos.append({
                            'title': title,
                            'url': f"https://www.youtube.com/watch?v={video_id}",
                            'views': view_count_text
                        })
            except Exception as e:
                logger.error(f"Error extracting YouTube data: {str(e)}")
        
        # ุทุฑููุฉ ุจุฏููุฉ ุฃุฎูุฑุฉ ูู ุญุงูุฉ ูุดู ุงูุทุฑู ุงูุณุงุจูุฉ
        if not videos:
            # ุงุณุชุฎุฏุงู ุชุนุจูุฑ ููุชุธู ููุจุญุซ ุนู ุฑูุงุจุท ุงูููุฏูู
            video_links = re.findall(r'href=\"\/watch\?v=([^\"]+)\"', response.text)
            seen_ids = set()
            
            for video_id in video_links:
                if video_id in seen_ids:
                    continue
                seen_ids.add(video_id)
                
                videos.append({
                    'title': f"ููุฏูู ุชุนูููู ุนู {query}",
                    'url': f"https://www.youtube.com/watch?v={video_id}",
                    'views': "ุบูุฑ ูุนุฑูู"
                })
        
        # ุงุฎุชูุงุฑ ุงูููุฏูููุงุช ุงูุฃูุซุฑ ูุดุงูุฏุฉ
        videos = videos[:5]  # ุฃุฎุฐ ุฃูู 5 ููุฏูููุงุช ููุชุตููุฉ
        
        # ูุญุงููุฉ ุชุฑุชูุจ ุญุณุจ ุงููุดุงูุฏุงุช ุฅุฐุง ูุงูุช ูุชููุฑุฉ
        try:
            videos = sorted(videos, key=lambda x: int(''.join(filter(str.isdigit, x['views'])) or 0), reverse=True)
        except:
            # ูู ุญุงูุฉ ูุดู ุงูุชุฑุชูุจุ ูุณุชุฎุฏู ุงูุชุฑุชูุจ ุงูุญุงูู
            pass
            
        return videos[:2]  # ุฅุฑุฌุงุน ุฃุนูู ููุฏููููู ูู ุญูุซ ุนุฏุฏ ุงููุดุงูุฏุงุช
        
    except Exception as e:
        logger.error(f"Error in YouTube search: {str(e)}")
        return []

def search_priority_sources(query):
    """ุงูุจุญุซ ูู ุงููุตุงุฏุฑ ุฐุงุช ุงูุฃููููุฉ"""
    results = []
    
    for name, url in PRIORITY_SOURCES.items():
        try:
            search_url = url + quote(query)
            response = requests.get(search_url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                links = []
                
                # ุฅุณุชุฑุงุชูุฌูุงุช ุจุญุซ ูุฎุตุตุฉ ุญุณุจ ุงููููุน
                if "dzexams" in url.lower():
                    # ุฎุงุต ุจูููุน DzExams
                    articles = soup.select('article.post')
                    for article in articles[:3]:
                        title_elem = article.select_one('h2.entry-title a')
                        if title_elem:
                            title = title_elem.text.strip()
                            href = title_elem['href']
                            links.append(f"{title}\n{href}")
                            
                elif "eddirasa" in url.lower():
                    # ุฎุงุต ุจูููุน eddirasa
                    results_div = soup.select('div.search-results article')
                    for article in results_div[:3]:
                        title_elem = article.select_one('h3 a')
                        if title_elem:
                            title = title_elem.text.strip()
                            href = title_elem['href']
                            links.append(f"{title}\n{href}")
                else:
                    # ุงูุจุญุซ ุงูุนุงู ููููุงูุน ุงูุฃุฎุฑู
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        if any(kw in href.lower() for kw in ["cours", "article", "4am", "examen", "ุฏุฑุณ"]):
                            title = link.text.strip()[:100]
                            if title and not any(ext in href for ext in ['.pdf', '.doc']):
                                links.append(f"{title}\n{href}")
                
                if links:
                    results.append((name, "\n".join(links[:2])))
        except Exception as e:
            logger.error(f"Error searching {name}: {str(e)}")
            continue
    
    return results

def search_all_sources(query):
    """ุงูุจุญุซ ูู ุฌููุน ุงููุตุงุฏุฑ ุงููุชุงุญุฉ"""
    # ุฃููุงู: ุงูุจุญุซ ูู ุงููุตุงุฏุฑ ุฐุงุช ุงูุฃููููุฉ
    results = search_priority_sources(query)
    
    # ุซุงููุงู: ุงูุจุญุซ ูู ุงููุตุงุฏุฑ ุงูุซุงูููุฉ
    for name, url in SECONDARY_SOURCES.items():
        try:
            search_url = url + quote(query)
            response = requests.get(search_url, timeout=3, headers={'User-Agent': 'Mozilla/5.0'})
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                links = []
                
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if any(kw in href.lower() for kw in ["cours", "article", "4am", "examen", "ุฏุฑุณ"]):
                        title = link.text.strip()[:100]
                        if title and not any(ext in href for ext in ['.pdf', '.doc']):
                            links.append(f"{title}\n{href}")
                
                if links:
                    results.append((name, "\n".join(links[:2])))
        except Exception as e:
            logger.error(f"Error searching {name}: {str(e)}")
            continue
    
    # ุซุงูุซุงู: ุงูุจุญุซ ูู ุงููุตุงุฏุฑ ุงูุฎุงุตุฉ
    for url in SPECIAL_SOURCES:
        try:
            response = requests.get(url, timeout=3)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                found_links = []
                
                for link in soup.find_all('a', href=True):
                    if query.split()[0].lower() in link.text.lower():
                        full_url = requests.compat.urljoin(url, link['href'])
                        found_links.append(f"{link.text.strip()}\n{full_url}")
                
                if found_links:
                    domain = url.split('//')[1].split('/')[0]
                    results.append((domain, "\n".join(found_links[:2])))
        except Exception as e:
            logger.error(f"Error with {url}: {str(e)}")
            continue
    
    return results

def get_ai_response(query):
    """ูุญุงููุฉ ุงูุญุตูู ุนูู ุฅุฌุงุจุฉ ูู ูููุฐุฌ AI ุจุฏูู ูู ุญุงูุฉ ูุดู ุฌููููุงู"""
    try:
        # ูุญุงููุฉ ุงุณุชุฎุฏุงู ุฌููููุงู API ุฃููุงู
        headers = {"Content-Type": "application/json"}
        data = {
            "contents": [{
                "parts": [{
                    "text": f"ุฃุฌุจ ุจุฏูุฉ ููุนูู ุฌุฒุงุฆุฑู ูุชุฎุตุต ูู ูููุงุฌ ุงูุณูุฉ ุงูุฑุงุจุนุฉ ูุชูุณุท: {query}"
                }]
            }]
        }
        
        try:
            response = requests.post(
                f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={API_KEY}",
                json=data,
                headers=headers,
                timeout=5
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    return result['candidates'][0]['content']['parts'][0]['text']
        except Exception as e:
            logger.error(f"Gemini API Error: {str(e)}")
        
        # ุฅุฐุง ูุดู ุฌููููุงูุ ูุณุชุฎุฏู ุจุฏูู ูุญูู ุจุณูุท (ูุงุนุฏุฉ ุฅุฌุงุจุงุช ูุณุจูุฉ)
        common_responses = {
            "ูุนุงุฏูุฉ": "ูู ุงูุณูุฉ ุงูุฑุงุจุนุฉ ูุชูุณุทุ ุชุฏุฑุณ ุงููุนุงุฏูุงุช ูู ุงูุฏุฑุฌุฉ ุงูุฃููู ุจูุฌููู ูุงุญุฏ. ุงููุงููู ุงูุฃุณุงุณู: ax + b = 0 ุญูุซ ุงูุญู x = -b/a",
            "ููุฏุณุฉ": "ุชุดูู ุงูููุฏุณุฉ ูู ุงูุณูุฉ ุงูุฑุงุจุนุฉ ูุชูุณุท ูุธุฑูุฉ ููุซุงุบูุฑุณุ ูุจุฑููุฉ ุทุงููุณุ ุงูุชุดุงุจูุ ูุงูุชูุงุณุจ ูู ุงููุซูุซุงุช.",
            "ููุซุงุบูุฑุณ": "ูุธุฑูุฉ ููุซุงุบูุฑุณ: ูู ูุซูุซ ูุงุฆู ุงูุฒุงููุฉุ ูุฑุจุน ุทูู ุงููุชุฑ ูุณุงูู ูุฌููุน ูุฑุจุนู ุทููู ุงูุถูุนูู ุงูุขุฎุฑูู. aยฒ + bยฒ = cยฒ",
            "ุทุงููุณ": "ูุจุฑููุฉ ุทุงููุณ: ุฅุฐุง ูุทุน ูุณุชูููุงู ูุชูุงุฒูุงู ูุทุนุชุงู ุนูู ูุณุชูููููุ ูุฅู ุงููุณุจุฉ ุจูู ููุงุณู ูุทุนุชูู ูู ุงุญุฏู ุงููุทุน ุชุณุงูู ุงููุณุจุฉ ุจูู ููุงุณู ุงููุทุนุชูู ูู ุงููุทุน ุงูุฃุฎุฑู.",
            "ุงุฎุชุจุงุฑ": "ุชุชููู ุงุฎุชุจุงุฑุงุช ุงูุฑูุงุถูุงุช ููุณูุฉ ุงูุฑุงุจุนุฉ ูุชูุณุท ุนุงุฏุฉ ูู ุซูุงุซุฉ ุชูุงุฑูู: ุชูุฑูู ุญูู ุงููุนุงุฏูุงุช ูุงูุญุณุงุจ ุงูุฌุจุฑูุ ุชูุฑูู ูู ุงูููุฏุณุฉุ ูุชูุฑูู ูู ุงูุฅุญุตุงุก ูุงูุงุญุชูุงูุงุช.",
            "ูุฑูุณูุฉ": "ูู ุงููุบุฉ ุงููุฑูุณูุฉ ููุณูุฉ ุงูุฑุงุจุนุฉ ูุชูุณุทุ ุชุฏุฑุณ ุงูููุงุนุฏ ุงูุฃุณุงุณูุฉ ูุซู ุงูุฃุฒููุฉ (Prรฉsent
